{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "## I. Task Description\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "\n",
    "Construct a predictive model to predict the target.\n",
    "\n",
    "The test set does not contain the target, so you will need to split up the train set accordingly into your own validation set. The test set supplied here is used to rank and assess your model on the competition leaderboard\n",
    "Your model will be evaluated as follows: for the top-250 cases predicted by your model, we count the number of hits (true positives), i.e. precision@250. We focus on the top-250 only as there's only a limited amount of time and resources to investigate risky customers. AUC is also shown on the leaderboard for informational purposes\n",
    "There are more than 500 churners present in the test set, though note that only 50% of the test set is used for the public leaderboard\n",
    "The large degree of class imbalance, the way how the target was defined, the way how your model is evaluated, and possibilities for feature engineering will make for challenges to overcome\n",
    "Instructions:\n",
    "\n",
    "The goal is not to \"win\", but to help you reflect on your model's results, see how others are doing, etc.\n",
    "The leaderboard is based on the number of true positives in the top-250 cases. The results of your latest submission are used to rank\n",
    "This means it is your job to keep track of different model versions / approaches / outputs in case you'd like to go back to an earlier result\n",
    "The \"public\" leaderboard calculates your score for a predetermined set 50% of the instances in the test set\n",
    "Later on, the leaderboard will be frozen (you'll be warned in advance) and the other 50% results (hidden leaderboard) will be shown\n",
    "You should then reflect on both results and explain accordingly in your report. E.g. if you did well on the public leaderboard but not on the hidden one, what might have caused this?\n",
    "Also take some time the reflect on the score measure being used here. Is the the measure you'd have chosen in this setting? How does the score compare to the AUC of your model?\n",
    "Reflect on the features and target definition. What would you change? How would your model be able to do better? Do you agree with the way the target was established or not?\n",
    "Your model needs to be build using R, Python (or Julia, if you so prefer). As an environment, you can use e.g. Jupyter (Notebook or Lab), RStudio, Google Colaboratory, Microsoft Azure Machine Learning Studio... and any additional library or package you want\n",
    "The first part of your lab report should contain a clear overview of your whole modeling pipeline, including approach, exploratory analysis (if any), preprocessing, construction of model, set-up of validation and results of the model:\n",
    "\n",
    "Feel free to include code fragments, tables, visualisations, etc.\n",
    "You can use any predictive technique/approach you want. The focus is not on the absolute outcome, but rather on the whole process: general setup, critical thinking, and the ability to get and validate an outcome\n",
    "You're free to use unsupervised technique for your data exploration part, too. When you decide to build a black box model, including some interpretability techniques to explain it is a plus\n",
    "Any other assumptions or insights are thoughts can be included as well: the idea is to take what we've seen in class, get your hands dirty and reflect on what we've seen\n",
    "Important: All groups should submit the results of their predictive model at least once to the leaderboard.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## II. Features \n",
    "\n",
    "Features:\n",
    "\n",
    "client_id: unique identifier per customer (anonymized hash); not to be used as a feature \n",
    "\n",
    "homebanking_active: whether the customer used / logged into home banking (Internet or Mobile app) this month\n",
    "\n",
    "has_homebanking: whether the customer has activated home banking (Internet of Mobile app)\n",
    "\n",
    "has_insurance_21: whether the customer owns \"tak 21\" life insurance products\n",
    "\n",
    "has_insurance_23: whether the customer owns \"tak 23\" life insurance products\n",
    "\n",
    "has_life_insurance_fixed_cap: whether the customer owns life insurance with fixed capital\n",
    "\n",
    "has_life_insurance_decreasing_cap: whether the customer owns life insurance with decreasing capital\n",
    "\n",
    "has_fire_car_other_insurance: whether the customer has fire/car/other insurance\n",
    "\n",
    "has_personal_loan: whether the customer has an outstanding personal loan\n",
    "\n",
    "has_mortgage_loan: whether the customer has an outstanding mortgage loan\n",
    "\n",
    "has_current_account: whether the customer has a current (checkings) account\n",
    "\n",
    "has_pension_saving: whether the customer has a pension (retirement) savings account\n",
    "\n",
    "has_savings_account: whether the customer has a savings account\n",
    "\n",
    "has_current_account_starter: whether the customer has a \"starter\" (a product offering some discounts and typically offered to new or younger customers) current (checkings) account\n",
    "\n",
    "has_savings_account_starter: whether the customer has a \"starter\" (a product offering some discounts and typically offered to new or younger customers) savings account\n",
    "\n",
    "bal_insurance_21: balance on \"tak 21\" life insurance\n",
    "\n",
    "bal_insurance_23: balance on \"tak 23\" life insurance\n",
    "\n",
    "cap_life_insurance_fixed_cap: capital for life insurance with fixed capital\n",
    "\n",
    "cap_life_insurance_decreasing_cap: capital for life insurance with decreasing capital\n",
    "\n",
    "prem_fire_car_other_insurance: premiums paid for fire/car/other insurance\n",
    "\n",
    "bal_personal_loan: outstanding balance on personal loans\n",
    "\n",
    "bal_mortgage_loan: outstanding balance on mortgage loans\n",
    "\n",
    "bal_current_account: balance on current (checkings) accounts\n",
    "\n",
    "bal_pension_saving: balance on pension (retirement) savings accounts\n",
    "\n",
    "bal_savings_account: balance on savings accounts\n",
    "\n",
    "bal_current_account_starter: balance on starter current (checkings) accounts\n",
    "\n",
    "bal_savings_account_starter: balance on starter savings accounts\n",
    "\n",
    "visits_distinct_so: how many different sales offices were visited in the past month by this customer (number of visits per office is not available)\n",
    "\n",
    "visits_distinct_so_areas: how many different sales office areas were visited in the past month by this customer\n",
    "\n",
    "customer_since_all: since when has the customer been a client for either bank or insurance products (YYYY-MM)\n",
    "\n",
    "customer_since_bank: since when has the customer been a client for bank products (YYYY-MM)\n",
    "\n",
    "customer_gender: gender code\n",
    "\n",
    "customer_birth_date: date of birth (YYYY-MM)\n",
    "\n",
    "customer_postal_code: postal code where the customer lives\n",
    "\n",
    "customer_occupation_code: occupation (job) code\n",
    "\n",
    "customer_self_employed: whether the customer is self-employed\n",
    "\n",
    "customer_education: code of education level\n",
    "\n",
    "customer_children: this describes family situation: either no children (no), one baby or toddler (onebaby), preschool child(ren) (preschool), children in secondary school (young), children in high school (adolescent), children between 18-24 (mature), children older than 24 (grownup), or children but without further details (yes)\n",
    "\n",
    "customer_relationship: this describes the marital situation: single (single), living together or married (couple). Note that divorced persons and widowers are also indicated as single\n",
    "\n",
    "target: target (1 or 0)\n",
    "\n",
    "\n",
    "Monetary values were rounded at tens of euros. Extremely affluent customers or customers with large negative balances were not included. Missing values are represented as \"NA\". Given the real-life nature of the data set, expect other data quality issues as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Data Analysis\n",
    "#### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import random as rd\n",
    "import sklearn.model_selection as sk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test files\n",
    "test_month_1 = pd.read_csv('data/test_month_1.csv')\n",
    "test_month_2 = pd.read_csv('data/test_month_2.csv')\n",
    "test_month_3 = pd.read_csv('data/test_month_3.csv')\n",
    "\n",
    "# train files\n",
    "train_month_1 = pd.read_csv('data/train_month_1.csv')\n",
    "train_month_2 = pd.read_csv('data/train_month_2.csv')\n",
    "train_month_3_with_target = pd.read_csv('data/train_month_3_with_target.csv')\n",
    "\n",
    "original = train_month_3_with_target.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Clean Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dates into days before 2018 \n",
    "train_month_3_with_target[\"customer_since_all\"] = (pd.to_datetime(\"2018-01\") - pd.to_datetime(train_month_3_with_target[\"customer_since_all\"])).dt.days\n",
    "train_month_3_with_target[\"customer_birth_date\"] = (pd.to_datetime(\"2018-01\") - pd.to_datetime(train_month_3_with_target[\"customer_birth_date\"])).dt.days\n",
    "train_month_3_with_target[\"customer_since_bank\"] = (pd.to_datetime(\"2018-01\") - pd.to_datetime(train_month_3_with_target[\"customer_since_bank\"])).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummies for the different categorical variables \n",
    "\n",
    "train_month_3_with_target = pd.DataFrame(pd.get_dummies(train_month_3_with_target, \n",
    "                                   columns = [\"customer_education\", \"customer_occupation_code\", \"customer_children\", \"customer_relationship\"], \n",
    "                                   dummy_na=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# maybe aggregate postal code into regions? dummies result in too many columns\n",
    "# maybe coordinates or distances from major cities in Belgium? \n",
    "# remove postal code for first model \n",
    "\n",
    "train_month_3_with_target = train_month_3_with_target.drop([\"customer_postal_code\"], axis=1)\n",
    "train_month_3_with_target = train_month_3_with_target.drop([\"client_id\"], axis=1)\n",
    "\n",
    "train_month_3_with_target['customer_since_all_na'] = np.where(train_month_3_with_target.customer_since_all.notnull(), '0', '1')\n",
    "train_month_3_with_target['customer_since_bank_na'] = np.where(train_month_3_with_target.customer_since_bank.notnull(), '0', '1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split to prevent data pollution\n",
    "y = train_month_3_with_target['target']\n",
    "X = train_month_3_with_target.drop(['target'], axis = 1)\n",
    "X_train, X_val, y_train, y_val = sk.train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "# create one complete set for testing purposes \n",
    "fortest = pd.concat([X_train.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at missing values\n",
    "miss_perc = pd.DataFrame()\n",
    "missings = original[[\"customer_since_all\", \"customer_since_bank\", \"customer_occupation_code\", \"customer_education\", \n",
    "                               \"customer_children\", \"customer_relationship\", \"target\"]].copy()\n",
    "\n",
    "miss_perc[\"percentage_missing\"] = missings.isna().sum(axis=0) / missings.shape[0]\n",
    "\n",
    "miss_perc[\"total_missing\"] =  missings.isna().sum(axis=0)\n",
    "miss_perc[\"missing_on_positive\"] = missings.loc[missings[\"target\"] == 1].isna().sum(axis=0)\n",
    "\n",
    "miss_perc[\"percentage_positive_missing\"] = miss_perc[\"missing_on_positive\"] / miss_perc[\"total_missing\"]\n",
    "\n",
    "miss_perc[\"ratio\"] = miss_perc[\"percentage_positive_missing\"] / miss_perc[\"percentage_missing\"]\n",
    "\n",
    "miss_perc = miss_perc[[\"percentage_missing\", \"percentage_positive_missing\", \"ratio\"]]\n",
    "\n",
    "# apparantly the customer_since... variables have comparatively a lot of NAs when our target variable is positive\n",
    "# We seemingly have almost always very personal information like education, children and relationship about positives\n",
    "# seems illogical to me => churners are well known?? \n",
    "\n",
    "miss_perc[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Descriptives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# distribution of target variable\n",
    "print(fortest[\"target\"].sum() / fortest.shape[0])\n",
    "\n",
    "# summary stats of numerical variables \n",
    "# only children and relationship are categorical \n",
    "fortest.describe()\n",
    "\n",
    "# summary stats of children and relationship\n",
    "\n",
    "\n",
    "# plot some histograms \n",
    "\n",
    "# remove the ids and target or reveiling variables for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missings in the training data \n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer = imputer.fit(X_train)\n",
    "X_train = imputer.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 3D\n",
    "\n",
    "X = StandardScaler().fit_transform(X_train)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "principalComponents = pca.fit_transform(X)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents , columns = ['PCA0', 'PCA1', 'PCA2'])\n",
    "result = pd.concat([principalDf, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['target'] = pd.Categorical(result['target'])\n",
    "my_color=result['target'].cat.codes\n",
    "\n",
    "fig = plt.figure(figsize=(30,16))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(result['PCA0'], result['PCA1'], result['PCA2'], c=my_color, cmap=\"Purples\", s=1)\n",
    " \n",
    "# make simple, bare axis lines through space:\n",
    "xAxisLine = ((min(result['PCA0']), max(result['PCA0'])), (0, 0), (0,0))\n",
    "ax.plot(xAxisLine[0], xAxisLine[1], xAxisLine[2], 'r')\n",
    "yAxisLine = ((0, 0), (min(result['PCA1']), max(result['PCA1'])), (0,0))\n",
    "ax.plot(yAxisLine[0], yAxisLine[1], yAxisLine[2], 'r')\n",
    "zAxisLine = ((0, 0), (0,0), (min(result['PCA2']), max(result['PCA2'])))\n",
    "ax.plot(zAxisLine[0], zAxisLine[1], zAxisLine[2], 'r')\n",
    " \n",
    "# label the axes\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_zlabel(\"PC3\")\n",
    "ax.set_title(\"PCA on the churn data set\")\n",
    "ax.view_init(0, 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 2D\n",
    "\n",
    "X = StandardScaler().fit_transform(X_train)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(X)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents , columns = ['principal component 1', 'principal component 2'])\n",
    "finalDf = pd.concat([principalDf.reset_index().drop(['index'], axis=1), y_train.reset_index().drop(['index'], axis=1)], axis = 1)\n",
    "\n",
    "# fit knn and show insample performance \n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=2)\n",
    "model.fit(principalDf,y_train)\n",
    "predicted = model.predict_proba(principalDf)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.672"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDf[\"prediction\"] = pd.DataFrame(predicted).iloc[:, 1]\n",
    "\n",
    "finalDf.sort_values(by=[\"prediction\"], inplace=True, ascending=False)\n",
    "\n",
    "finalDf.head(250)[\"target\"].sum(axis = 0) / 250 \n",
    "\n",
    "# knn seems to work but oos performance questionable \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the information \n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = [ 0, 1]\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['target'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 1)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find positives where PC2 < -1 and PC 1<0 \n",
    "\n",
    "trues = finalDf[(finalDf['principal component 1'] <= 0)]['target'].sum()\n",
    "total_space = finalDf[(finalDf['principal component 1'] <= 0) & (finalDf['principal component 2'])]['target'].shape[0]\n",
    "print(total_space)\n",
    "trues / total_space\n",
    "\n",
    "# around 2% of the bottom left corner contains positives in a clusteres area \n",
    "# maybe try to apply KNN here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# selection of the transformed data \n",
    "\n",
    "finalDft = finalDf[(finalDf['principal component 1'] <= 0)]\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = [ 0, 1]\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDft['target'] == target\n",
    "    ax.scatter(finalDft.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDft.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 1)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Do something with new data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate all steps for the validation set \n",
    "\n",
    "# imputation of NAs\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer = imputer.fit(X_val)\n",
    "X_val = imputer.transform(X_val)\n",
    "\n",
    "# apply PCA transformation from training\n",
    "newdata_transformed = pca.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = pd.DataFrame(X_val.isna().sum(axis=0))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
